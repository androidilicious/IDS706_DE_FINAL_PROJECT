# ğŸ“‹ Project Summary

## Quick Reference

### Run Complete Pipeline
```bash
cd ingestion
python ingestion_pipeline.py
```

### Test Before Running
```bash
cd ingestion
python test_connections.py
```

### Project Structure
```
IDS706_DE_FINAL_PROJECT/
â”œâ”€â”€ .env                        # Credentials (DO NOT COMMIT)
â”œâ”€â”€ .env.example                # Credentials template
â”œâ”€â”€ .gitignore                  # Git exclusions
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Main documentation
â”‚
â”œâ”€â”€ .kaggle/                    # Kaggle API credentials
â”‚   â””â”€â”€ kaggle.json             # API token (DO NOT COMMIT)
â”‚
â”œâ”€â”€ ingestion/                  # Data pipeline
â”‚   â”œâ”€â”€ README.md               # Pipeline documentation
â”‚   â”œâ”€â”€ ingestion_pipeline.py   # Main orchestrator â­
â”‚   â”œâ”€â”€ download_from_kaggle.py # Kaggle downloader
â”‚   â”œâ”€â”€ upload_to_s3.py         # S3 uploader
â”‚   â”œâ”€â”€ create_schema.py        # Schema manager
â”‚   â”œâ”€â”€ s3_to_rds.py            # Data loader
â”‚   â”œâ”€â”€ run_s3_to_postgres.py   # Simplified loader
â”‚   â”œâ”€â”€ test_connections.py     # Connection validator
â”‚   â””â”€â”€ schema_raw.sql          # DDL script
â”‚
â””â”€â”€ notebooks/                  # Analysis notebooks
    â””â”€â”€ eda_olist.ipynb         # EDA
```

## Pipeline Flow

```
Kaggle API â†’ Local CSV â†’ AWS S3 â†’ PostgreSQL (Aiven)
   â†“              â†“         â†“            â†“
9 files       120 MB    Raw Zone    9 tables
                                   1.5M rows
```

## Database Schema

| Table | Rows | Dependencies |
|-------|------|--------------|
| customers_raw | 99,441 | None |
| sellers_raw | 3,095 | None |
| products_raw | 32,951 | None |
| geolocation_raw | 1,000,163 | None |
| product_category_name_translation_raw | 71 | None |
| orders_raw | 99,441 | customers_raw |
| order_items_raw | 112,650 | orders_raw, products_raw, sellers_raw |
| order_payments_raw | 103,886 | orders_raw |
| order_reviews_raw | 99,224 | orders_raw |

**Total: 1,551,022 rows**

## Key Features

âœ… **Automated**: End-to-end pipeline from Kaggle to PostgreSQL  
âœ… **Idempotent**: Safe to run multiple times  
âœ… **Smart**: Skips unchanged data (incremental updates)  
âœ… **Robust**: Foreign key handling, duplicate detection  
âœ… **Monitored**: Progress tracking and detailed logs  
âœ… **Secure**: All credentials in .env file  

## Configuration Files

### .env (Root)
```env
# AWS
AWS_ACCESS_KEY_ID=xxx
AWS_SECRET_ACCESS_KEY=xxx
S3_BUCKET=de-27-team3
S3_PREFIX=raw/
S3_REGION=us-east-2

# PostgreSQL
DB_HOST=xxx.aivencloud.com
DB_PORT=22446
DB_NAME=defaultdb
DB_USER=avnadmin
DB_PASSWORD=xxx
```

### .kaggle/kaggle.json (Root)
```json
{
  "username": "your_username",
  "key": "your_api_key"
}
```

Get from: https://www.kaggle.com/account

## Common Tasks

### First-Time Setup
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure Kaggle
# Place kaggle.json in .kaggle/ folder

# 3. Test connections
cd ingestion
python test_connections.py

# 4. Run pipeline
python ingestion_pipeline.py
```

### Daily Operations
```bash
# Quick data refresh (if already in S3)
cd ingestion
python run_s3_to_postgres.py

# Full refresh (from Kaggle)
python ingestion_pipeline.py

# Force schema recreation
python ingestion_pipeline.py --force-schema-recreate
```

### Troubleshooting
```bash
# Test connections
python test_connections.py

# Check S3 contents
aws s3 ls s3://de-27-team3/raw/

# Check database
python -c "from test_connections import test_postgres_connection; test_postgres_connection()"
```

## Performance Metrics

- **Download time**: ~2-3 minutes
- **S3 upload**: ~1-2 minutes
- **Database load**: ~3-5 minutes
- **Total runtime**: ~6-10 minutes

## Data Quality

âœ… NaN values â†’ NULL  
âœ… Duplicates handled (ON CONFLICT)  
âœ… Foreign keys enforced  
âœ… DOUBLE PRECISION for large numbers  
âœ… Progress tracking for large tables  

## Team

- **Pinaki Ghosh** - Data Engineering & Orchestration
- **Austin Zhang** - Data Transformation & Modeling
- **Diwas Puri** - Data Ingestion & Storage
- **Michael Badu** - Testing & Analytics

## Resources

- **Dataset**: [Olist Brazilian E-Commerce](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
- **S3 Bucket**: de-27-team3 (us-east-2)
- **Database**: Aiven PostgreSQL (pg-3729bd9d-bnbgoals.j.aivencloud.com)
- **Repository**: IDS706_DE_FINAL_PROJECT

## Next Steps

1. âœ… Data ingestion complete
2. ğŸ”„ Create transformed tables (analytics layer)
3. ğŸ”„ Build dashboards
4. ğŸ”„ Implement data quality checks
5. ğŸ”„ Schedule automated runs
6. ğŸ”„ Document analytical queries

---

**Last Updated**: December 2, 2024  
**Pipeline Status**: âœ… Fully Operational
